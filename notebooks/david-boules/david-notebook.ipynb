{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cd9fb13-6f30-447d-b3f8-8f720b5f0393",
   "metadata": {},
   "source": [
    "# Phase 1: Introduction to the Project & Dataset Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6e61e1-20a4-4186-b4b2-22ca48d62faa",
   "metadata": {},
   "source": [
    "### Brief Description of the Dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b68abde-16ed-4b93-b74d-03beb5052c26",
   "metadata": {},
   "source": [
    "Link to the Dataset: https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia\n",
    "\n",
    "#### <u>The dataset is comprised of 3 folders:<u>\n",
    "\n",
    "1. **train** - *contains the images that the model will be trained on* (the training set)\n",
    "   - **1341 images** are x-rays of **'normal'** lungs, **3875 images** are x-rays of lungs with **pneumonia**\n",
    "2. **test** - *contains the images that the model will be tested on to evaluate its performance, providing guidelines for model tuning* (the test set)\n",
    "   - **234 images** are x-rays of **'normal'** lungs, **390 images** are x-rays of lungs with **pneumonia**\n",
    "3. **val** - *contains the images that will be used to assess the final performance of the fully trained model* (the validation set)\n",
    "   - **8 images** are x-rays of **'normal'** lungs, **8 images** are x-rays of lungs with **pneumonia**\n",
    "\n",
    "The dataset contains images of lungs with both 'bacterial' and 'viral' pneumonia, however the model will not differentiate between the two (it will simply aim to classify a given x-ray image of a lung as being infected with pneumonia or not)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb990ca-6a02-40e2-837f-69b69cd18ff9",
   "metadata": {},
   "source": [
    "### **Pointing out the class imbalance**\n",
    "With a total of 5216 images in the training set, roughly **74.3%** of the images in the training set are **x-ray images of pneumonia-infected lungs**, leaving ***only around 25.7%*** of the images in the training set to be ***x-ray images of healthy lungs***. **This is quite a significant class imbalance** (almost 3x as many pneumonia infected lungs as healthy lungs) and this will need to be handled later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b1c03a-4f19-44b9-8593-db0238f112e7",
   "metadata": {},
   "source": [
    "# Phase 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509ed2b6-1aee-4101-b499-07a83dc89a8b",
   "metadata": {},
   "source": [
    "### Importing Tensorflow & the ImageDataGenerator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cd7482-e60f-431b-89db-8a5585bfbac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce13619-32a3-4df7-a152-269ddb7c0e66",
   "metadata": {},
   "source": [
    "## 1. Preprocessing the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32062ff9-d851-445e-99cd-4a3897189090",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('data/train',\n",
    "                                                 target_size = (64,64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 color_mode = 'grayscale',\n",
    "                                                 class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c4ca64-a291-48e6-a593-c2327278c61d",
   "metadata": {},
   "source": [
    "Keras' *flow_from_directory* expects a directory structure whereby the main directory (which is passed in as the first parameter) contains **subdirectories** for each class, which is how this dataset is structured. Images are then assigned a label according to the folder name (due to alphabetical order, images in 'NORMAL' are assigned 0, images in 'PNEUMONIA' are assigned 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0457e8a1-af08-4628-b0b3-25a582e8e10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "images, labels = next(training_set)\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.imshow(images[i], cmap='gray')\n",
    "    plt.title(f\"Label: {int(labels[i])}\") #0 for normal, 1 for pneumonia\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74e69c7-abf4-4e36-a1d6-63ba8c4ef9d3",
   "metadata": {},
   "source": [
    "## 2. Preprocessing the Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c628f9c-6f74-4fa5-81fb-55abf02da207",
   "metadata": {},
   "source": [
    "The test set should **not be augmented** because it should reflect *real-world conditions*, the model would be tested on **altered images** that aren't actual medical scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4af4735-5e8a-478a-b65c-d04b13f2bed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('data/test',\n",
    "                                                 target_size = (64,64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 color_mode = 'grayscale',\n",
    "                                                 class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207f9775-f519-4372-91c3-407d996d9ecd",
   "metadata": {},
   "source": [
    "#### Handling the Class Imbalance: Using Class Weighting instead of Resampling techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56392a87-6b7c-4f61-8356-4d9ec84cbb8a",
   "metadata": {},
   "source": [
    "Since the class imbalance is not too extreme, and resampling (over/undersampling) can lead to overfitting/loss of valuable information, I will proceed with *cost-sensitive learning*, by using sklearn's **compute_class_weight** function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55afb4a-9fcc-4fbf-ba19-763f55c219fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "class_labels = np.array([0,1]) #(0 = Normal, 1 = Pneumonia)\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight = \"balanced\",\n",
    "    classes = class_labels,\n",
    "    y = [0]*1341 + [1]*3875)\n",
    "\n",
    "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "print(\"Computed Class Weights:\", class_weight_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2812e5c-9c8c-4622-ba4e-3584c8078b12",
   "metadata": {},
   "source": [
    "The CNN model to be built calculates a **loss function** which the model aims to minimize throughout subsequent epocs. With class weighting, the loss function applies **higher penalties** to misclassified samples from the *minority class* (which are the normal lungs). This handles the class imbalance by urging the model to pay more attention to the minority class (normal lungs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7d6741-ecd0-446b-9cbe-1f216a6eb4ff",
   "metadata": {},
   "source": [
    "# Phase 3: Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03b76d5-7a3e-4b84-ba38-2d5a4daf9bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5a4da6-8e56-4936-a4eb-1a7cccfe84f0",
   "metadata": {},
   "source": [
    "### 1. Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac914267-7ea2-42dd-9324-59f6cbce563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1bdf2d-ed4b-4a5d-b64d-a70d97f0e13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Conv2D(filters=32,\n",
    "               kernel_size=3,\n",
    "               activation='relu',\n",
    "               input_shape=(64,64,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a16e952-52c4-4c67-b911-92114801942c",
   "metadata": {},
   "source": [
    "### 2. Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f0895b-83b5-4dba-97b0-55639ad70324",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(MaxPool2D(pool_size=2,\n",
    "                  strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab78651-c089-40a9-b2e2-6c5d110e7548",
   "metadata": {},
   "source": [
    "**Multiple convolution layers may be added to make the architecture more complex** - This is needed since detecting pneumonia in grayscale x-ray images is already very difficult for the human eye: The more difficult it is to detect certain the features, the greater the need for a more complex model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b24f2c-d233-4154-a261-95d81f2e8055",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second Layer:\n",
    "cnn.add(Conv2D(filters=32,\n",
    "                 kernel_size=3,\n",
    "                 activation='relu'))\n",
    "\n",
    "cnn.add(MaxPool2D(pool_size=2,\n",
    "                  strides=2))\n",
    "\n",
    "#Third Layer:\n",
    "cnn.add(Conv2D(filters=32,\n",
    "                 kernel_size=3,\n",
    "                 activation='relu'))\n",
    "\n",
    "cnn.add(MaxPool2D(pool_size=2,\n",
    "                  strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2020d34f-1655-4dc7-8fdd-ef1bb2bbc1ea",
   "metadata": {},
   "source": [
    "### 3. Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafd504c-8c1e-414e-a71e-a88934ee5f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c025569-c720-4469-a589-42888beed697",
   "metadata": {},
   "source": [
    "### 4. Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0096e941-e669-4c94-b0c8-67b2d6d9af2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Dense(units=128,\n",
    "              activation='relu'))\n",
    "\n",
    "#final output layer\n",
    "cnn.add(Dense(units=1,\n",
    "              activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1382f1dd-52e2-4e60-8f92-ca3e887a7ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics=['accuracy', Precision(name='precision'), Recall(name='recall')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437f3aa3-eae9-4c48-b813-43af21f60df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = cnn.fit(x=training_set, validation_data=test_set, epochs=10, class_weight = class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2abc9b-2c97-4fd1-8e46-6344f9f19eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd1ab97-f157-43ac-9387-3c91c65b009f",
   "metadata": {},
   "source": [
    "There are relatively erratic fluctuations in validation loss. While the fluctuations are not too significant in number, there is no \n",
    "clear downward trend: The model is clearly struggling to generalize.\n",
    "\n",
    "The model also seems to be overfitting due to **lower validation accuracy** than training accuracy, as well as **higher validation loss** than training loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119ace57-6e0d-4304-a652-8070d600d563",
   "metadata": {},
   "source": [
    "### Precision Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e465ad41-bd1d-4f27-9b76-854be63b2ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Precision:\", history.history['precision'], '\\n')\n",
    "print(\"Validation Precision:\", history.history['val_precision'], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889bd298-6ddf-4fcc-82b3-ac9273aec8c3",
   "metadata": {},
   "source": [
    "The model seems to be **slightly weak** at minimizing false positives when it comes to unseen data (validation precision is quite significantly lower than training precision across all epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb69aa6-ce78-4916-9c08-7402f8ed0f29",
   "metadata": {},
   "source": [
    "### Recall Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6241fe0-5c20-46e5-a64d-580ca3088a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Recall:\", history.history['recall'], '\\n')\n",
    "print(\"Validation Recall:\", history.history['val_recall'], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fec140-f6fc-41eb-92ab-c4d0053fdef3",
   "metadata": {},
   "source": [
    "The model seems to be **relatively strong** at detecting actual pneumonia cases when it comes to unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651e28e4-7f91-4a90-8cc9-3e74a6b8c329",
   "metadata": {},
   "source": [
    "## Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64f82db-d185-4e22-8f8b-b70a5138b614",
   "metadata": {},
   "source": [
    "### Trying a lower learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfb7e70-f731-4fa3-81d0-4a8c55aa4f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default learning rate of adam optimizer is 0.001, try 0.0005\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "adam_optimizer = Adam(learning_rate=0.0005)\n",
    "\n",
    "cnn.compile(optimizer = adam_optimizer, loss = 'binary_crossentropy', metrics=['accuracy', Precision(name='precision'), Recall(name='recall')])\n",
    "history = cnn.fit(x=training_set, validation_data=test_set, epochs=15, class_weight=class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b3407b-a98d-4c91-968f-88970582e62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1292e9c1-5aed-4099-935b-6431fcb13323",
   "metadata": {},
   "source": [
    "There are slight upward/downward trends in validation accuracy/loss respectively, however there are still erratic fluctuations - **slight improvement nonetheless**\n",
    "\n",
    "This may suggest batch sensitivity and/or model instability - overall this shows the model is still inconsistent. Additionally, the model is still overfitting due to the significant difference between training and validation accuracy/loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea731005-5f99-4c90-9b55-cb04fdfd6f9a",
   "metadata": {},
   "source": [
    "### Trying a larger batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebeddc3-b93e-4ec7-b98b-b2fd2986577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try larger batch size\n",
    "training_set = train_datagen.flow_from_directory('data/train',\n",
    "                                                 target_size = (64,64),\n",
    "                                                 batch_size = 64,\n",
    "                                                 color_mode = 'grayscale',\n",
    "                                                 class_mode = 'binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('data/test',\n",
    "                                                 target_size = (64,64),\n",
    "                                                 batch_size = 64,\n",
    "                                                 color_mode = 'grayscale',\n",
    "                                                 class_mode = 'binary')\n",
    "\n",
    "cnn.compile(optimizer = adam_optimizer, loss = 'binary_crossentropy', metrics=['accuracy', Precision(name='precision'), Recall(name='recall')])\n",
    "history = cnn.fit(x=training_set, validation_data=test_set, epochs=15, class_weight=class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b424547-5902-4e0e-b74b-def554617b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee515317-7b9d-4476-aef4-fc73f9b59920",
   "metadata": {},
   "source": [
    "Model seems even more unstable: This may have occured since larger batch sizes lead to **less frequent weight updates** (the model's weights update after each batch, therefore larger bathces means fewer batches = fewer updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fd4f51-e678-486d-9bfa-665bc901fe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reducing batch size back to 32\n",
    "training_set = train_datagen.flow_from_directory('data/train',\n",
    "                                                 target_size = (64,64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 color_mode = 'grayscale',\n",
    "                                                 class_mode = 'binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('data/test',\n",
    "                                                 target_size = (64,64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 color_mode = 'grayscale',\n",
    "                                                 class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0091bbc-fbba-41fa-8bdc-48957a060515",
   "metadata": {},
   "source": [
    "### Trying an even lower learning rate and introducing early stopping and L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5574a631-bd0f-4e9a-84d5-9e8b3600c8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try lower learning rate + modify Early Stopping\n",
    "adam_optimizer = Adam(learning_rate=0.0003) #0.0005 --> 0.0003\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True) #Stop after 4 rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ac7d9f-288e-4d39-97bf-4422d8a78421",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Redefining model architecture with Regularization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "cnn = Sequential()\n",
    "\n",
    "# First Convolutional Layer\n",
    "cnn.add(Conv2D(32, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001), input_shape=(64, 64, 1)))\n",
    "cnn.add(MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "# Second Convolutional Layer\n",
    "cnn.add(Conv2D(32, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "cnn.add(MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "# Third Convolutional Layer\n",
    "cnn.add(Conv2D(32, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "cnn.add(MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "# Flatten layer\n",
    "cnn.add(Flatten())\n",
    "\n",
    "# Fully Connected Layer\n",
    "cnn.add(Dense(units=128, activation='relu'))\n",
    "\n",
    "#Output Layer\n",
    "cnn.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "cnn.compile(optimizer=adam_optimizer, loss='binary_crossentropy', metrics=['accuracy', Precision(name='precision'), Recall(name='recall')])\n",
    "cnn.fit(x=training_set, validation_data=test_set, epochs=20, class_weight=class_weight_dict, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f12f6a0-8c03-4439-85c0-4bc3995d52c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = cnn.history\n",
    "epochs = range(len(history.history['accuracy']))\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(epochs, history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training & Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, history.history['loss'], label='Training Loss')\n",
    "plt.plot(epochs, history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Training & Validation Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c65847c-a0b0-4d73-b2da-9bd887a6dc64",
   "metadata": {},
   "source": [
    "#### Excluding observation of 1st epoch (usually has instability due to untrained weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3da953-42ec-453e-9cd5-31371d1c2029",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(history.history['accuracy'])) #one less epoch\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, history.history['accuracy'][1:], label='Training Accuracy')\n",
    "plt.plot(epochs, history.history['val_accuracy'][1:], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training & Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, history.history['loss'][1:], label='Training Loss')\n",
    "plt.plot(epochs, history.history['val_loss'][1:], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Training & Validation Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7148b984-652d-4311-9713-510d9ccf4afe",
   "metadata": {},
   "source": [
    "**Significant improvement**. Model looks more stable (although there are still some erratic changes in validation accuracy/loss), less overfitting (smaller difference betweeen train and validation accuracy) and there are clear upward/downward trends for validation accuracy/loss respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c256a3fe-3b27-4253-b6f0-91403785f23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Precision:\", history.history['precision'], '\\n')\n",
    "print(\"Validation Precision:\", history.history['val_precision'], '\\n')\n",
    "\n",
    "print(\"Training Recall:\", history.history['recall'], '\\n')\n",
    "print(\"Validation Recall:\", history.history['val_recall'], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef67e5f-73e7-4cc4-a184-abbbe77d5d3f",
   "metadata": {},
   "source": [
    "## Making a Single Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b32c29-dff4-4e11-8060-41b56751f212",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "\n",
    "img = image.load_img('data/val/NORMAL/NORMAL2-IM-1442-0001.jpeg', target_size=(64, 64), color_mode='grayscale')\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "img_array = img_array / 255.0\n",
    "\n",
    "prediction = cnn.predict(img_array)\n",
    "class_label = \"Pneumonia\" if prediction > 0.5 else \"Normal\"\n",
    "\n",
    "print(f\"Prediction: {class_label} (Confidence: {prediction[0][0]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639c4a48-5aea-44e0-8521-9c804ba25432",
   "metadata": {},
   "source": [
    "- Model Predicts 5/8 Normal Lungs **correctly** in the Validation Set\n",
    "- Model Predicts 8/8 Pneumonia Lungs **correctly** in the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11495d81-6289-4355-ab88-c5db60868e19",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfda92be-ba39-492f-9e0a-f5823135cd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.save(\"../../web-app/david-boules/model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb60aff-1c72-4084-9008-5f1beeb8bb5b",
   "metadata": {},
   "source": [
    "# References:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2de50e-b9ec-4020-a0cc-3549da6b6ad6",
   "metadata": {},
   "source": [
    "- Data Augmentation Techniques: https://www.linkedin.com/advice/0/how-do-you-implement-data-augmentation-techniques\n",
    "- Handling Class Imbalance in Image Classification: Techniques and Best Practices: https://medium.com/@okeshakarunarathne/handling-class-imbalance-in-image-classification-techniques-and-best-practices-c539214440b0\n",
    "- Handling Class Imbalances using Class Weights: https://medium.com/@ravi.abhinav4/improving-class-imbalance-with-class-weights-in-machine-learning-af072fdd4aa4\n",
    "- The Ultimate Guide to Convolutional Neural Networks: https://www.superdatascience.com/blogs/the-ultimate-guide-to-convolutional-neural-networks-cnn\n",
    "- Building a Convolutional Neural Network using TensorFlow: https://www.analyticsvidhya.com/blog/2021/06/building-a-convolutional-neural-network-using-tensorflow-keras/\n",
    "- Fixing Overfitting on a CNN: https://www.geeksforgeeks.org/what-are-the-possible-approaches-to-fixing-overfitting-on-a-cnn/\n",
    "- Regularization in Deep Learning: https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
