{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "waiting-chase",
   "metadata": {},
   "source": [
    "## SDS 21 - Pneumonia detection of X-ray imaging using CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-surgery",
   "metadata": {},
   "source": [
    "Steps in Implementing CNN\n",
    "- Import required libraries and packages\n",
    "- Test Data path -> chest_xray\\test\\Normal,Pneumonia\n",
    "- Train Data Path -> chest_xray\\train\\Normal,Pneumonia\n",
    "- Validation Data Path -> chest_xray\\val\\Normal,Pneumonia\n",
    "- Set data locations\n",
    "- Labelling dataset (normal or pneumonia)\n",
    "- Count the number of images per folder to see if the dataset is balanced or not\n",
    "- Data Augmentation to handle the imbalance\n",
    "- Oversampling minority class (Handle Imbalance)\n",
    "- Compute class weights to handle imbalance\n",
    "- CNN Architecture, Build layers\n",
    "- Compile model with adam optimizer\n",
    "- Define callbacks to setup checkpoints or have early stops\n",
    "- Train model\n",
    "- Evaluate performance on test set using Precision, Recall and F1-score\n",
    "- Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "maritime-charity",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries and packages\n",
    "# We are going to use Tensorflow, scikit learn for metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import classification_report,precision_score, recall_score, f1_score\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "hollywood-incentive",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Data locations\n",
    "#Data is moved out of the solution folder and the detailed paths are provided, instead of short path\n",
    "train_path = r'C:\\\\Users\\\\ghk23\\OneDrive\\\\XPS_OneDrive\\\\Databases\\\\chest_xray\\\\train'\n",
    "test_path = 'C:/Users/ghk23/OneDrive/XPS_OneDrive/Databases/chest_xray/test'\n",
    "val_path = r'C:/Users/ghk23/OneDrive/XPS_OneDrive/Databases/chest_xray/val'\n",
    "data_gen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "instant-supply",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Data Image Count Summary\n",
      "Original count: NORMAL = 1341, PNEUMONIA = 3875\n"
     ]
    }
   ],
   "source": [
    "# Labelling Dataset - Set class indices\n",
    "#We need to tell the model the classes and the binary options avaiable.\n",
    "class_indices = train_data.class_indices  # {'NORMAL': 0, 'PNEUMONIA': 1}\n",
    "normal_idx = class_indices['NORMAL']\n",
    "pneumonia_idx = class_indices['PNEUMONIA']\n",
    "\n",
    "# Per Category, count the number of images.\n",
    "num_normal = sum(1 for _, label in zip(train_data.filepaths, train_data.classes) if label == normal_idx)\n",
    "num_pneumonia = sum(1 for _, label in zip(train_data.filepaths, train_data.classes) if label == pneumonia_idx)\n",
    "\n",
    "print(f\"\\nTrain Data Image Count Summary\\nOriginal count: NORMAL = {num_normal}, PNEUMONIA = {num_pneumonia}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-transcription",
   "metadata": {},
   "source": [
    "This shows that the dataset is imbalanced. \n",
    "Now, we need to implement certain balancing measures.\n",
    "We are going to use three methods to achieve the same:\n",
    "- Data Augmentation (Train Dataset)\n",
    "- By Oversampling minority class\n",
    "- Compute class weights to handle imbalance dataset - OPTIONAL (With Class Weights, Precision was 78%, Recall was 99% and F1-score was 86%. Based on the result when removed, it may or may not be included in the end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "technical-momentum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation to handle class imbalance and improve generalization\n",
    "#Data Augmentation is one of the simplest and easiest ways to reduce the imbalance.\n",
    "data_gen = ImageDataGenerator(\n",
    "    rescale=1./255,      # Normalize pixel values to [0,1] - Converting image data to pixel data\n",
    "    rotation_range=30,   # Rotate images by up to 30 degrees\n",
    "    horizontal_flip=True,  # Randomly flip images horizontally\n",
    "    zoom_range=0.2,  # Zoom in and out by 10%\n",
    "    brightness_range=[0.8, 1.2],  # Adjust brightness between 80% and 120%\n",
    "    fill_mode='nearest' #Fill missing pixels\n",
    ")\n",
    "\n",
    "# Load training data with augmentations\n",
    "train_data = data_gen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=(224, 224), #Used sharper images than the 150 ones to make the artifacts more pronounced\n",
    "    batch_size=32,\n",
    "    class_mode='binary', #Binary class mode as we have only normal and pneumonia classes\n",
    "    color_mode='grayscale', #Images are grayscale and not RGB\n",
    "    shuffle=True) #Shuffle the images - Default is true\n",
    "\n",
    "# Load testing data without augmentation (only rescaling)\n",
    "test_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "test_data = test_data_gen.flow_from_directory(\n",
    "    test_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    color_mode='grayscale',\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-roberts",
   "metadata": {},
   "source": [
    "- We will first identify based on our indexes which one is the minority class.\n",
    "- Next, we will obtain the number of samples to unsample. This is nothing but the delta between the number of normal images and\n",
    "- The number of Pneumonia images. So, the different will be our target number to unsample.\n",
    "- For the minority class, we will generate these many images in memory.\n",
    "- Oversampling minority class is the best way to handle imbalance than the undersampling majority class. \n",
    "- The reason for this is to avoid underfitting and loss of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "mediterranean-highlight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling done: Added 2534 images to balance the dataset\n"
     ]
    }
   ],
   "source": [
    "# Handling class imbalance by oversampling minority class\n",
    "if num_normal > num_pneumonia:\n",
    "    minority_class = pneumonia_idx\n",
    "    majority_class = normal_idx\n",
    "else:\n",
    "    minority_class = normal_idx\n",
    "    majority_class = pneumonia_idx\n",
    "\n",
    "# Identify the number of samples to upsample\n",
    "num_to_upsample = abs(num_normal - num_pneumonia)\n",
    "\n",
    "# Generate additional images for the minority class\n",
    "minority_images = [train_data.filepaths[i] for i in range(len(train_data.filepaths)) if train_data.classes[i] == minority_class]\n",
    "upsampled_images = resample(minority_images, replace=True, n_samples=num_to_upsample, random_state=42)\n",
    "\n",
    "print(f\"Oversampling done: Added {len(upsampled_images)} images to balance the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "native-commonwealth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated count: NORMAL = 3875, PNEUMONIA = 3875\n"
     ]
    }
   ],
   "source": [
    "#Recount the images and print (To check if we did the oversampling in the right way)\n",
    "num_normal_updated = num_normal + (num_to_upsample if minority_class == normal_idx else 0)\n",
    "num_pneumonia_updated = num_pneumonia + (num_to_upsample if minority_class == pneumonia_idx else 0)\n",
    "\n",
    "print(f\"Updated count: NORMAL = {num_normal_updated}, PNEUMONIA = {num_pneumonia_updated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "failing-secondary",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initially implemented, but may not be needed\n",
    "#Compute Class weights to handle imbalance\n",
    "# class_weights = compute_class_weight('balanced', classes=np.unique(train_data.classes), y=train_data.classes)\n",
    "# class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "# print(f\"Computed class weights: {class_weight_dict}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-savage",
   "metadata": {},
   "source": [
    "### CNN Architecture, Model Building, Training, Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cardiac-understanding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build CNN model with multiple convolutional and pooling layers\n",
    "#Here a 50% dropout number is provided. This turns-off 50% of the neurons and thereby improves the performance. \n",
    "#It reduces the dependency on a few weights and reduces the chance of overfitting\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(224, 224, 1)),\n",
    "    MaxPooling2D(2,2),\n",
    "    \n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    \n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dangerous-lewis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model with Adam optimizer and binary cross-entropy loss function\n",
    "# Adam sets the learning rate. Slow learning rate like 0.0001 will converge the output slowly. This is good for accuracy.\n",
    "# As this is a binary classification probel, binary cross entropy is used and accuracy is used to show it in the output.\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "complicated-recall",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks to save best model and stop early if no improvement\n",
    "# Set intermediatry checkpoints (ONly if it is the best model)\n",
    "# So, model will stop after 5 epochs with no improvement.\n",
    "checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "activated-ferry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "163/163 [==============================] - ETA: 0s - loss: 0.2852 - accuracy: 0.8742"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ghk23\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/163 [==============================] - 220s 1s/step - loss: 0.2852 - accuracy: 0.8742 - val_loss: 0.2960 - val_accuracy: 0.8654\n",
      "Epoch 2/10\n",
      "163/163 [==============================] - 231s 1s/step - loss: 0.2672 - accuracy: 0.8848 - val_loss: 0.2661 - val_accuracy: 0.8926\n",
      "Epoch 3/10\n",
      "163/163 [==============================] - 234s 1s/step - loss: 0.2371 - accuracy: 0.9053 - val_loss: 0.2540 - val_accuracy: 0.9022\n",
      "Epoch 4/10\n",
      "163/163 [==============================] - 230s 1s/step - loss: 0.2257 - accuracy: 0.9072 - val_loss: 0.2489 - val_accuracy: 0.9087\n",
      "Epoch 5/10\n",
      "163/163 [==============================] - 230s 1s/step - loss: 0.2072 - accuracy: 0.9160 - val_loss: 0.2568 - val_accuracy: 0.8990\n",
      "Epoch 6/10\n",
      "163/163 [==============================] - 232s 1s/step - loss: 0.1909 - accuracy: 0.9233 - val_loss: 0.2584 - val_accuracy: 0.8942\n",
      "Epoch 7/10\n",
      "163/163 [==============================] - 231s 1s/step - loss: 0.1855 - accuracy: 0.9254 - val_loss: 0.3096 - val_accuracy: 0.8590\n",
      "Epoch 8/10\n",
      "163/163 [==============================] - 227s 1s/step - loss: 0.1854 - accuracy: 0.9233 - val_loss: 0.3058 - val_accuracy: 0.8510\n",
      "Epoch 9/10\n",
      "163/163 [==============================] - 230s 1s/step - loss: 0.1958 - accuracy: 0.9197 - val_loss: 0.3810 - val_accuracy: 0.8173\n"
     ]
    }
   ],
   "source": [
    "# Train model using the training dataset with validation monitoring\n",
    "history = model.fit(train_data, validation_data=test_data, epochs=10, class_weight=class_weight_dict, callbacks=[checkpoint, early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cloudy-geography",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 321ms/step\n",
      "1/1 [==============================] - 0s 214ms/step\n",
      "1/1 [==============================] - 0s 258ms/step\n",
      "1/1 [==============================] - 0s 246ms/step\n",
      "1/1 [==============================] - 0s 276ms/step\n",
      "1/1 [==============================] - 0s 252ms/step\n",
      "1/1 [==============================] - 0s 273ms/step\n",
      "1/1 [==============================] - 0s 270ms/step\n",
      "1/1 [==============================] - 0s 264ms/step\n",
      "1/1 [==============================] - 0s 256ms/step\n",
      "1/1 [==============================] - 0s 235ms/step\n",
      "1/1 [==============================] - 0s 203ms/step\n",
      "1/1 [==============================] - 0s 196ms/step\n",
      "1/1 [==============================] - 0s 200ms/step\n",
      "1/1 [==============================] - 0s 192ms/step\n",
      "1/1 [==============================] - 0s 200ms/step\n",
      "1/1 [==============================] - 0s 191ms/step\n",
      "1/1 [==============================] - 0s 220ms/step\n",
      "1/1 [==============================] - 0s 215ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      NORMAL       0.86      0.90      0.88       234\n",
      "   PNEUMONIA       0.94      0.92      0.93       390\n",
      "\n",
      "    accuracy                           0.91       624\n",
      "   macro avg       0.90      0.91      0.90       624\n",
      "weighted avg       0.91      0.91      0.91       624\n",
      "\n",
      "Precision: 0.9370\n",
      "Recall: 0.9154\n",
      "F1-Score: 0.9261\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model performance on test set using precision, recall, and F1-score\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "# Iterate through test dataset to get predictions\n",
    "for images, labels in test_data:\n",
    "    preds = model.predict(images)\n",
    "    y_true.extend(labels)\n",
    "    y_pred.extend(preds.round())\n",
    "    if len(y_true) >= test_data.samples:\n",
    "        break\n",
    "\n",
    "# Compute and print classification metrics\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=['NORMAL', 'PNEUMONIA']))\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1-Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "flying-flood",
   "metadata": {},
   "outputs": [],
   "source": [
    "#When medical imaging is the use case, high recall should be favoured. \n",
    "#Also, there is an overall balance in the model with a good F1-score.\n",
    "# Save the trained model to a file for later reuse\n",
    "model.save('pneumonia_cnn_model.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
