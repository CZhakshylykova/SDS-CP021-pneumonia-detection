{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chest X-ray Pneumonia Detection: Exploratory Data Analysis & Data Preprocessing\n",
    "#### Dataset: https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia\n",
    "#### Author: Cholpon Zhakshylykova\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cholponzhakshylykova/Desktop/SDS/SDS-CP021-pneumonia-detection/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#---------------Import Libraries and Setup\n",
    "\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/paultimothymooney/chest-xray-pneumonia?dataset_version_number=2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2.29G/2.29G [02:51<00:00, 14.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n",
      "DATA_ROOT: /Users/cholponzhakshylykova/.cache/kagglehub/datasets/paultimothymooney/chest-xray-pneumonia/versions/2/chest_xray\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Configuration and Data Download\n",
    "# Download the dataset using kagglehub\n",
    "dataset_dir = kagglehub.dataset_download(\"paultimothymooney/chest-xray-pneumonia\")\n",
    "DATA_ROOT = os.path.join(dataset_dir, \"chest_xray\")\n",
    "print(\"DATA_ROOT:\", DATA_ROOT)\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "os.makedirs(\"plots\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Define ChestXrayEDA Class\n",
    "class ChestXrayEDA:\n",
    "    \"\"\"Comprehensive EDA class for Chest X-ray Pneumonia dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, data_root: str):\n",
    "        self.data_root = Path(data_root)\n",
    "        self.splits = ['train', 'val', 'test']\n",
    "        self.classes = ['NORMAL', 'PNEUMONIA']\n",
    "        self.dataset_stats = {}\n",
    "        self._validate_dataset_structure()\n",
    "    \n",
    "    def _validate_dataset_structure(self):\n",
    "        \"\"\"Validate that the dataset has the expected structure\"\"\"\n",
    "        if not self.data_root.exists():\n",
    "            raise FileNotFoundError(f\"Dataset root not found: {self.data_root}\")\n",
    "        \n",
    "        for split in self.splits:\n",
    "            split_path = self.data_root / split\n",
    "            if not split_path.exists():\n",
    "                raise FileNotFoundError(f\"Split directory not found: {split_path}\")\n",
    "            \n",
    "            for cls in self.classes:\n",
    "                class_path = split_path / cls\n",
    "                if not class_path.exists():\n",
    "                    raise FileNotFoundError(f\"Class directory not found: {class_path}\")\n",
    "    \n",
    "    def analyze_dataset_distribution(self) -> Dict:\n",
    "        \"\"\"Analyze the distribution of images across splits and classes\"\"\"\n",
    "        stats = {}\n",
    "        \n",
    "        for split in self.splits:\n",
    "            stats[split] = {}\n",
    "            split_path = self.data_root / split\n",
    "            \n",
    "            for cls in self.classes:\n",
    "                class_path = split_path / cls\n",
    "                image_files = [f for f in class_path.iterdir() if f.suffix.lower() in ['.jpg', '.jpeg', '.png']]\n",
    "                stats[split][cls] = len(image_files)\n",
    "            \n",
    "            stats[split]['total'] = sum(stats[split].values())\n",
    "        \n",
    "        self.dataset_stats = stats\n",
    "\n",
    "        print(\"=\"*60)\n",
    "        print(\"DATASET DISTRIBUTION ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for split in self.splits:\n",
    "            print(f\"\\n{split.upper()} SET:\")\n",
    "            for cls in self.classes:\n",
    "                count = stats[split][cls]\n",
    "                percentage = (count / stats[split]['total']) * 100\n",
    "                print(f\"  {cls:>10}: {count:>5} images ({percentage:.1f}%)\")\n",
    "            print(f\"  {'TOTAL':>10}: {stats[split]['total']:>5} images\")\n",
    "        \n",
    "        # Overall statistics\n",
    "        total_images = sum(stats[split]['total'] for split in self.splits)\n",
    "        total_normal = sum(stats[split]['NORMAL'] for split in self.splits)\n",
    "        total_pneumonia = sum(stats[split]['PNEUMONIA'] for split in self.splits)\n",
    "        \n",
    "        print(f\"\\nOVERALL DATASET:\")\n",
    "        print(f\"  {'NORMAL':>10}: {total_normal:>5} images ({(total_normal/total_images)*100:.1f}%)\")\n",
    "        print(f\"  {'PNEUMONIA':>10}: {total_pneumonia:>5} images ({(total_pneumonia/total_images)*100:.1f}%)\")\n",
    "        print(f\"  {'TOTAL':>10}: {total_images:>5} images\")\n",
    "        \n",
    "        # Class imbalance analysis\n",
    "        imbalance_info = []\n",
    "        imbalance_threshold = 1.2\n",
    "        \n",
    "        for split in self.splits:\n",
    "            n_normal = stats[split]['NORMAL']\n",
    "            n_pneumonia = stats[split]['PNEUMONIA']\n",
    "            ratio = max(n_normal, n_pneumonia) / (min(n_normal, n_pneumonia) + 1e-9)\n",
    "            imbalance_info.append((split, ratio))\n",
    "        \n",
    "        overall_ratio = max(total_normal, total_pneumonia) / (min(total_normal, total_pneumonia) + 1e-9)\n",
    "        \n",
    "        print(\"\\nCLASS IMBALANCE ANALYSIS & RECOMMENDATION:\")\n",
    "        for split, ratio in imbalance_info:\n",
    "            print(f\"  {split.upper()} set imbalance ratio: {ratio:.2f} (max/min)\")\n",
    "        print(f\"  OVERALL imbalance ratio: {overall_ratio:.2f} (max/min)\")\n",
    "        \n",
    "        if overall_ratio > imbalance_threshold:\n",
    "            print(\"\\nRecommendation: There is a significant class imbalance.\")\n",
    "            print(\"It is recommended to use OVERSAMPLING (or class weighting) during model training to address this.\")\n",
    "        else:\n",
    "            print(\"\\nNo significant class imbalance detected.\")\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def visualize_distribution(self):\n",
    "        \"\"\"Create comprehensive visualizations of dataset distribution\"\"\"\n",
    "        if not self.dataset_stats:\n",
    "            self.analyze_dataset_distribution()\n",
    "        \n",
    "        splits = list(self.dataset_stats.keys())\n",
    "        normal_counts = [self.dataset_stats[split]['NORMAL'] for split in splits]\n",
    "        pneumonia_counts = [self.dataset_stats[split]['PNEUMONIA'] for split in splits]\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Stacked bar chart\n",
    "        x = np.arange(len(splits))\n",
    "        width = 0.6\n",
    "        \n",
    "        axes[0, 0].bar(x, normal_counts, width, label='NORMAL', alpha=0.8)\n",
    "        axes[0, 0].bar(x, pneumonia_counts, width, bottom=normal_counts, label='PNEUMONIA', alpha=0.8)\n",
    "        axes[0, 0].set_xlabel('Dataset Split')\n",
    "        axes[0, 0].set_ylabel('Number of Images')\n",
    "        axes[0, 0].set_title('Dataset Distribution by Split (Stacked)')\n",
    "        axes[0, 0].set_xticks(x)\n",
    "        axes[0, 0].set_xticklabels([s.capitalize() for s in splits])\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Grouped bar chart\n",
    "        width = 0.35\n",
    "        axes[0, 1].bar(x - width/2, normal_counts, width, label='NORMAL', alpha=0.8)\n",
    "        axes[0, 1].bar(x + width/2, pneumonia_counts, width, label='PNEUMONIA', alpha=0.8)\n",
    "        axes[0, 1].set_xlabel('Dataset Split')\n",
    "        axes[0, 1].set_ylabel('Number of Images')\n",
    "        axes[0, 1].set_title('Class Distribution Comparison')\n",
    "        axes[0, 1].set_xticks(x)\n",
    "        axes[0, 1].set_xticklabels([s.capitalize() for s in splits])\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Overall pie chart\n",
    "        total_normal = sum(normal_counts)\n",
    "        total_pneumonia = sum(pneumonia_counts)\n",
    "        \n",
    "        axes[1, 0].pie([total_normal, total_pneumonia], labels=['NORMAL', 'PNEUMONIA'],\n",
    "                       autopct='%1.1f%%', startangle=90, colors=['lightblue', 'lightcoral'])\n",
    "        axes[1, 0].set_title('Overall Class Distribution')\n",
    "        \n",
    "        # Imbalance ratio chart\n",
    "        imbalance_ratios = []\n",
    "        split_labels = []\n",
    "        \n",
    "        for split in splits:\n",
    "            normal = self.dataset_stats[split]['NORMAL']\n",
    "            pneumonia = self.dataset_stats[split]['PNEUMONIA']\n",
    "            ratio = pneumonia / normal if normal > 0 else 0\n",
    "            imbalance_ratios.append(ratio)\n",
    "            split_labels.append(f\"{split.capitalize()}\\n({pneumonia}:{normal})\")\n",
    "        \n",
    "        bars = axes[1, 1].bar(range(len(splits)), imbalance_ratios, alpha=0.8)\n",
    "        axes[1, 1].set_xlabel('Dataset Split')\n",
    "        axes[1, 1].set_ylabel('Pneumonia:Normal Ratio')\n",
    "        axes[1, 1].set_title('Class Imbalance by Split')\n",
    "        axes[1, 1].set_xticks(range(len(splits)))\n",
    "        axes[1, 1].set_xticklabels(split_labels)\n",
    "        axes[1, 1].axhline(y=1, color='red', linestyle='--', alpha=0.7, label='Balanced')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for i, (bar, ratio) in enumerate(zip(bars, imbalance_ratios)):\n",
    "            axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                            f'{ratio:.2f}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        fig.savefig(os.path.join(\"plots\", \"dataset_distribution.png\"), dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def sample_images_visualization(self, n_samples: int = 8):\n",
    "        \"\"\"Display sample images from each class\"\"\"\n",
    "        fig, axes = plt.subplots(2, n_samples, figsize=(20, 8))\n",
    "        \n",
    "        for class_idx, class_name in enumerate(self.classes):\n",
    "            class_path = self.data_root / 'train' / class_name\n",
    "            image_files = list(class_path.glob('*.jpeg')) + list(class_path.glob('*.jpg'))\n",
    "            sampled_files = random.sample(image_files, min(n_samples, len(image_files)))\n",
    "            \n",
    "            for img_idx, img_path in enumerate(sampled_files):\n",
    "                try:\n",
    "                    img = Image.open(img_path).convert('L')\n",
    "                    axes[class_idx, img_idx].imshow(img, cmap='gray')\n",
    "                    axes[class_idx, img_idx].set_title(f'{class_name}\\n{img_path.name}', fontsize=10)\n",
    "                    axes[class_idx, img_idx].axis('off')\n",
    "                except Exception as e:\n",
    "                    axes[class_idx, img_idx].text(0.5, 0.5, f'Error loading\\n{img_path.name}',\n",
    "                                                 ha='center', va='center', transform=axes[class_idx, img_idx].transAxes)\n",
    "                    axes[class_idx, img_idx].axis('off')\n",
    "        \n",
    "        plt.suptitle('Sample Images from Each Class', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        fig.savefig(os.path.join(\"plots\", \"sample_images.png\"), dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def analyze_image_properties(self, sample_size: int = 100):\n",
    "        \"\"\"Analyze image properties like dimensions, file sizes, etc.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"IMAGE PROPERTIES ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        properties = {split: {cls: {'widths': [], 'heights': [], 'file_sizes': []} \n",
    "                             for cls in self.classes} for split in self.splits}\n",
    "        \n",
    "        for split in self.splits:\n",
    "            for cls in self.classes:\n",
    "                class_path = self.data_root / split / cls\n",
    "                image_files = list(class_path.glob('*.jpeg')) + list(class_path.glob('*.jpg'))\n",
    "                \n",
    "                # Sample random images for analysis\n",
    "                sample_files = random.sample(image_files, min(sample_size, len(image_files)))\n",
    "                \n",
    "                for img_path in sample_files:\n",
    "                    try:\n",
    "                        with Image.open(img_path) as img:\n",
    "                            width, height = img.size\n",
    "                            properties[split][cls]['widths'].append(width)\n",
    "                            properties[split][cls]['heights'].append(height)\n",
    "                            properties[split][cls]['file_sizes'].append(img_path.stat().st_size)\n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "        \n",
    "        # Display statistics\n",
    "        for split in self.splits:\n",
    "            print(f\"\\n{split.upper()} SET:\")\n",
    "            for cls in self.classes:\n",
    "                widths = properties[split][cls]['widths']\n",
    "                heights = properties[split][cls]['heights']\n",
    "                file_sizes = properties[split][cls]['file_sizes']\n",
    "                \n",
    "                if widths:  # Only if we have data\n",
    "                    print(f\"  {cls}:\")\n",
    "                    print(f\"    Width:  {np.mean(widths):.0f} ± {np.std(widths):.0f} px (range: {np.min(widths)}-{np.max(widths)})\")\n",
    "                    print(f\"    Height: {np.mean(heights):.0f} ± {np.std(heights):.0f} px (range: {np.min(heights)}-{np.max(heights)})\")\n",
    "                    print(f\"    Size:   {np.mean(file_sizes)/1024:.1f} ± {np.std(file_sizes)/1024:.1f} KB\")\n",
    "        \n",
    "        return properties\n",
    "    \n",
    "    def plot_image_properties(self, properties: Dict):\n",
    "        \"\"\"Plot image properties distribution\"\"\"\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        \n",
    "        # Collect all data for plotting\n",
    "        all_widths = {'NORMAL': [], 'PNEUMONIA': []}\n",
    "        all_heights = {'NORMAL': [], 'PNEUMONIA': []}\n",
    "        all_file_sizes = {'NORMAL': [], 'PNEUMONIA': []}\n",
    "        \n",
    "        for split in self.splits:\n",
    "            for cls in self.classes:\n",
    "                all_widths[cls].extend(properties[split][cls]['widths'])\n",
    "                all_heights[cls].extend(properties[split][cls]['heights'])\n",
    "                all_file_sizes[cls].extend(properties[split][cls]['file_sizes'])\n",
    "        \n",
    "        # Width distribution\n",
    "        axes[0, 0].hist([all_widths['NORMAL'], all_widths['PNEUMONIA']], \n",
    "                       bins=30, alpha=0.7, label=['NORMAL', 'PNEUMONIA'])\n",
    "        axes[0, 0].set_xlabel('Width (pixels)')\n",
    "        axes[0, 0].set_ylabel('Frequency')\n",
    "        axes[0, 0].set_title('Image Width Distribution')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(alpha=0.3)\n",
    "        \n",
    "        # Height distribution\n",
    "        axes[0, 1].hist([all_heights['NORMAL'], all_heights['PNEUMONIA']], \n",
    "                       bins=30, alpha=0.7, label=['NORMAL', 'PNEUMONIA'])\n",
    "        axes[0, 1].set_xlabel('Height (pixels)')\n",
    "        axes[0, 1].set_ylabel('Frequency')\n",
    "        axes[0, 1].set_title('Image Height Distribution')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(alpha=0.3)\n",
    "        \n",
    "        # File size distribution\n",
    "        axes[0, 2].hist([np.array(all_file_sizes['NORMAL'])/1024, np.array(all_file_sizes['PNEUMONIA'])/1024], \n",
    "                       bins=30, alpha=0.7, label=['NORMAL', 'PNEUMONIA'])\n",
    "        axes[0, 2].set_xlabel('File Size (KB)')\n",
    "        axes[0, 2].set_ylabel('Frequency')\n",
    "        axes[0, 2].set_title('File Size Distribution')\n",
    "        axes[0, 2].legend()\n",
    "        axes[0, 2].grid(alpha=0.3)\n",
    "        \n",
    "        # Aspect ratio analysis\n",
    "        aspect_ratios = {'NORMAL': [], 'PNEUMONIA': []}\n",
    "        for cls in self.classes:\n",
    "            for w, h in zip(all_widths[cls], all_heights[cls]):\n",
    "                if h > 0:\n",
    "                    aspect_ratios[cls].append(w/h)\n",
    "        \n",
    "        axes[1, 0].hist([aspect_ratios['NORMAL'], aspect_ratios['PNEUMONIA']], \n",
    "                       bins=30, alpha=0.7, label=['NORMAL', 'PNEUMONIA'])\n",
    "        axes[1, 0].set_xlabel('Aspect Ratio (W/H)')\n",
    "        axes[1, 0].set_ylabel('Frequency')\n",
    "        axes[1, 0].set_title('Aspect Ratio Distribution')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(alpha=0.3)\n",
    "        \n",
    "        # Box plots for dimensions\n",
    "        width_data = [all_widths['NORMAL'], all_widths['PNEUMONIA']]\n",
    "        height_data = [all_heights['NORMAL'], all_heights['PNEUMONIA']]\n",
    "        \n",
    "        axes[1, 1].boxplot(width_data, labels=['NORMAL', 'PNEUMONIA'])\n",
    "        axes[1, 1].set_ylabel('Width (pixels)')\n",
    "        axes[1, 1].set_title('Width Distribution (Box Plot)')\n",
    "        axes[1, 1].grid(alpha=0.3)\n",
    "        \n",
    "        axes[1, 2].boxplot(height_data, labels=['NORMAL', 'PNEUMONIA'])\n",
    "        axes[1, 2].set_ylabel('Height (pixels)')\n",
    "        axes[1, 2].set_title('Height Distribution (Box Plot)')\n",
    "        axes[1, 2].grid(alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        fig.savefig(os.path.join(\"plots\", \"image_properties.png\"), dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "print(\"✅ ChestXrayEDA class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Initialize EDA Object and Validate Dataset\n",
    "print(\"🫁 CHEST X-RAY PNEUMONIA DETECTION - EDA & PREPROCESSING\")\n",
    "print(\"=\" * 70)\n",
    "# Initialize EDA object\n",
    "eda = ChestXrayEDA(DATA_ROOT)\n",
    "print(\"✅ Dataset structure validated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Analyze Dataset Distribution\n",
    "print(\"\\n📊 ANALYZING DATASET DISTRIBUTION...\")\n",
    "dataset_stats = eda.analyze_dataset_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Visualize Dataset Distribution\n",
    "print(\"\\n🎨 CREATING DISTRIBUTION VISUALIZATIONS...\")\n",
    "eda.visualize_distribution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Display Sample Images\n",
    "print(\"\\n🖼️  DISPLAYING SAMPLE IMAGES...\")\n",
    "eda.sample_images_visualization(n_samples=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Analyze Image Properties\n",
    "print(\"\\n📏 ANALYZING IMAGE PROPERTIES...\")\n",
    "properties = eda.analyze_image_properties(sample_size=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Plot Image Properties\n",
    "print(\"\\n📈 PLOTTING IMAGE PROPERTIES...\")\n",
    "eda.plot_image_properties(properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Data Augmentation Visualization\n",
    "def visualize_augmentations(data_root: str, n_augmentations: int = 5):\n",
    "    \"\"\"Visualize data augmentation transformations\"\"\"\n",
    "    print(\"\\n🔄 DEMONSTRATING DATA AUGMENTATION...\")\n",
    "    \n",
    "    # Define augmentation transforms\n",
    "    transform_augment = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    # Select sample images from both classes\n",
    "    fig, axes = plt.subplots(2, n_augmentations + 1, figsize=(20, 8))\n",
    "    \n",
    "    for class_idx, class_name in enumerate(['NORMAL', 'PNEUMONIA']):\n",
    "        sample_path = Path(data_root) / 'train' / class_name\n",
    "        sample_files = list(sample_path.glob('*.jpeg')) + list(sample_path.glob('*.jpg'))\n",
    "        sample_img_path = random.choice(sample_files)\n",
    "        \n",
    "        # Load original image\n",
    "        original_img = Image.open(sample_img_path).convert('L')\n",
    "        \n",
    "        # Show original\n",
    "        axes[class_idx, 0].imshow(original_img, cmap='gray')\n",
    "        axes[class_idx, 0].set_title(f'Original\\n{class_name}')\n",
    "        axes[class_idx, 0].axis('off')\n",
    "        \n",
    "        # Show augmented versions\n",
    "        for i in range(n_augmentations):\n",
    "            augmented = transform_augment(original_img)\n",
    "            axes[class_idx, i + 1].imshow(augmented.squeeze(), cmap='gray')\n",
    "            axes[class_idx, i + 1].set_title(f'Augmented {i + 1}')\n",
    "            axes[class_idx, i + 1].axis('off')\n",
    "    \n",
    "    plt.suptitle('Data Augmentation Examples', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(os.path.join(\"plots\", \"data_augmentation.png\"), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Apply augmentation visualization\n",
    "visualize_augmentations(DATA_ROOT, n_augmentations=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Preprocessing Recommendations\n",
    "def preprocessing_recommendations(dataset_stats: Dict):\n",
    "    \"\"\"Provide preprocessing recommendations based on EDA findings\"\"\"\n",
    "    print(\"\\n💡 PREPROCESSING RECOMMENDATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Calculate class imbalance\n",
    "    total_normal = sum(dataset_stats[split]['NORMAL'] for split in dataset_stats.keys())\n",
    "    total_pneumonia = sum(dataset_stats[split]['PNEUMONIA'] for split in dataset_stats.keys())\n",
    "    imbalance_ratio = max(total_normal, total_pneumonia) / min(total_normal, total_pneumonia)\n",
    "    \n",
    "    print(\"1. CLASS IMBALANCE:\")\n",
    "    if imbalance_ratio > 1.5:\n",
    "        print(\"   ⚠️  Significant class imbalance detected\")\n",
    "        print(\"   📋 Recommendations:\")\n",
    "        print(\"      - Use weighted loss function (WeightedRandomSampler)\")\n",
    "        print(\"      - Apply SMOTE or oversampling techniques\")\n",
    "        print(\"      - Consider focal loss for training\")\n",
    "    else:\n",
    "        print(\"   ✅ Classes are relatively balanced\")\n",
    "    \n",
    "    print(\"\\n2. DATA AUGMENTATION:\")\n",
    "    print(\"   📋 Recommended augmentations:\")\n",
    "    print(\"      - Random rotation (±15°)\")\n",
    "    print(\"      - Random horizontal flip\")\n",
    "    print(\"      - Random brightness/contrast adjustment\")\n",
    "    print(\"      - Random zoom/scaling\")\n",
    "    print(\"      - Gaussian noise (medical imaging)\")\n",
    "    \n",
    "    print(\"\\n3. PREPROCESSING PIPELINE:\")\n",
    "    print(\"   📋 Recommended steps:\")\n",
    "    print(\"      - Resize to consistent dimensions (224x224 or 256x256)\")\n",
    "    print(\"      - Normalize pixel values to [0,1] or [-1,1]\")\n",
    "    print(\"      - Apply histogram equalization if needed\")\n",
    "    print(\"      - Consider lung segmentation for better focus\")\n",
    "    \n",
    "    print(\"\\n4. TRAIN/VALIDATION/TEST SPLIT:\")\n",
    "    train_total = dataset_stats['train']['total']\n",
    "    val_total = dataset_stats['val']['total']\n",
    "    test_total = dataset_stats['test']['total']\n",
    "    total_images = train_total + val_total + test_total\n",
    "    \n",
    "    print(f\"   Current split: {train_total/total_images:.1%} / {val_total/total_images:.1%} / {test_total/total_images:.1%}\")\n",
    "    \n",
    "    if val_total/total_images < 0.15:\n",
    "        print(\"   ⚠️  Validation set might be too small\")\n",
    "        print(\"   📋 Consider stratified sampling for better validation\")\n",
    "    \n",
    "    print(\"\\n5. MODEL CONSIDERATIONS:\")\n",
    "    print(\"   📋 Recommendations:\")\n",
    "    print(\"      - Use transfer learning (ResNet, DenseNet, EfficientNet)\")\n",
    "    print(\"      - Apply gradual unfreezing\")\n",
    "    print(\"      - Use appropriate metrics (AUC, F1-score, sensitivity, specificity)\")\n",
    "    print(\"      - Implement early stopping and learning rate scheduling\")\n",
    "\n",
    "# Generate recommendations\n",
    "preprocessing_recommendations(dataset_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Summary and Next Steps\n",
    "print(\"\\n🎯 SUMMARY AND NEXT STEPS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"✅ COMPLETED TASKS:\")\n",
    "print(\"   • Dataset structure validation\")\n",
    "print(\"   • Distribution analysis across splits and classes\")\n",
    "print(\"   • Image properties analysis\")\n",
    "print(\"   • Sample visualization\")\n",
    "print(\"   • Data augmentation demonstration\")\n",
    "print(\"   • Preprocessing recommendations\")\n",
    "\n",
    "print(\"\\n📁 GENERATED FILES:\")\n",
    "print(\"   • plots/dataset_distribution.png\")\n",
    "print(\"   • plots/sample_images.png\")\n",
    "print(\"   • plots/image_properties.png\")\n",
    "print(\"   • plots/data_augmentation.png\")\n",
    "\n",
    "print(\"\\n🚀 NEXT STEPS:\")\n",
    "print(\"   1. Implement data preprocessing pipeline\")\n",
    "print(\"   2. Create balanced data loaders\")\n",
    "print(\"   3. Design CNN architecture or use transfer learning\")\n",
    "print(\"   4. Implement training loop with proper validation\")\n",
    "print(\"   5. Evaluate model performance with medical metrics\")\n",
    "print(\"   6. Deploy model with proper validation and monitoring\")\n",
    "\n",
    "print(\"\\n✅ EDA COMPLETED SUCCESSFULLY!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
