{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Chest X-ray Pneumonia Classification with CNNs\n",
    "\n",
    "**Author:** Cholpon Zhakshylykova \n",
    "**Background:** M.Sc. Molecular Medicine  | Machine Learning Projects  \n",
    "**Date:** September 2025  \n",
    "\n",
    "\n",
    "##  Project Overview\n",
    "This notebook demonstrates the use of **Convolutional Neural Networks (CNNs)** \n",
    "to classify chest X-ray images as **Normal** or **Pneumonia**.  \n",
    "\n",
    "The dataset is from [Kaggle](https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia) and includes \n",
    "separate training, validation, and test splits.  \n",
    "\n",
    "\n",
    "##  Workflow\n",
    "1. Data Loading & Preprocessing (normalize, resize, augment)  \n",
    "2. CNN Model Training (PyTorch)  \n",
    "3. Evaluation (accuracy, F1, confusion matrix, ROC curve)  \n",
    "4. Visualization of learning curves  \n",
    "\n",
    "\n",
    "\n",
    "##  Results\n",
    "- Achieved 83% on the test set.  \n",
    "- Data augmentation improved generalization.  \n",
    "- Demonstrates the potential of deep learning in medical imaging.  \n",
    "\n",
    "\n",
    "\n",
    "##  Context\n",
    "This project is part of my **portfolio in Machine Learning for Healthcare**.  \n",
    "It showcases practical skills in:  \n",
    "- Exploratory Data Analysis (EDA)  \n",
    "- Deep Learning with PyTorch  \n",
    "- Medical image classification  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, models\n",
    "from PIL import Image\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    roc_auc_score, roc_curve, auc, f1_score\n",
    ")\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import matplotlib.pyplot as plt\n",
    "import kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seed for Reproducibility\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.12), please consider upgrading to the latest version (0.3.13).\n",
      "DATA_ROOT: /Users/cholponzhakshylykova/.cache/kagglehub/datasets/paultimothymooney/chest-xray-pneumonia/versions/2/chest_xray\n"
     ]
    }
   ],
   "source": [
    "# Config & Data Download\n",
    "\n",
    "dataset_dir = kagglehub.dataset_download(\"paultimothymooney/chest-xray-pneumonia\")\n",
    "DATA_ROOT = os.path.join(dataset_dir, \"chest_xray\")\n",
    "print(\"DATA_ROOT:\", DATA_ROOT)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 15\n",
    "PATIENCE = 4\n",
    "IMG_SIZE = 128\n",
    "NUM_CLASSES = 2\n",
    "LOG_DIR = \"runs/chest_xray\"\n",
    "REPORTS_DIR = \"reports\"\n",
    "PLOTS_DIR = \"plots\"\n",
    "\n",
    "for folder in [REPORTS_DIR, PLOTS_DIR]:\n",
    "    os.makedirs(folder, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Data Augmentation\n",
    "\n",
    "\n",
    "def to_1ch(x, **kwargs):\n",
    "    # If the image has shape (H, W, 3), pick the first channel only\n",
    "    if x.shape[-1] == 3:\n",
    "        x = x[..., 0:1]\n",
    "    return x\n",
    "\n",
    "class AlbumentationsTransform:\n",
    "    def __init__(self, aug):\n",
    "        self.aug = aug\n",
    "    def __call__(self, img):\n",
    "        return self.aug(image=np.array(img))['image']\n",
    "\n",
    "train_aug = A.Compose([\n",
    "    A.ToGray(p=1.0),\n",
    "    A.Lambda(image=to_1ch),\n",
    "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.Rotate(limit=15, p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.OneOf([\n",
    "        A.GaussianBlur(p=0.5),\n",
    "        A.MotionBlur(p=0.5)\n",
    "    ], p=0.2),\n",
    "    A.Normalize([0.5], [0.5]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "val_aug = A.Compose([\n",
    "    A.ToGray(p=1.0),\n",
    "    A.Lambda(image=to_1ch),     # Fix for validation/test transforms!\n",
    "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "    A.Normalize([0.5], [0.5]),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dataset & Dataloaders (+Oversampling)\n",
    "\n",
    "class OversampledDataset(Dataset):\n",
    "    def __init__(self, normal_files, pneumonia_files, transform=None):\n",
    "        n_normal, n_pneumonia = len(normal_files), len(pneumonia_files)\n",
    "        if n_normal < n_pneumonia:\n",
    "            normal_files = normal_files * (n_pneumonia // n_normal) + random.sample(normal_files, n_pneumonia % n_normal)\n",
    "        elif n_pneumonia < n_normal:\n",
    "            pneumonia_files = pneumonia_files * (n_normal // n_pneumonia) + random.sample(pneumonia_files, n_normal % n_pneumonia)\n",
    "        self.images = normal_files + pneumonia_files\n",
    "        self.labels = [0]*len(normal_files) + [1]*len(pneumonia_files)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self): return len(self.images)\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.images[idx]).convert('L')\n",
    "        if self.transform: img = self.transform(img)\n",
    "        return img, self.labels[idx]\n",
    "\n",
    "def get_dataloaders(data_root, batch_size=32):\n",
    "    train_normal = list((Path(data_root) / 'train' / 'NORMAL').glob('*.jpg')) + list((Path(data_root) / 'train' / 'NORMAL').glob('*.jpeg'))\n",
    "    train_pneu = list((Path(data_root) / 'train' / 'PNEUMONIA').glob('*.jpg')) + list((Path(data_root) / 'train' / 'PNEUMONIA').glob('*.jpeg'))\n",
    "\n",
    "    train_dataset = OversampledDataset(\n",
    "        [str(x) for x in train_normal],\n",
    "        [str(x) for x in train_pneu],\n",
    "        transform=AlbumentationsTransform(train_aug)\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    val_dataset = datasets.ImageFolder(Path(data_root) / 'val',\n",
    "                                      transform=AlbumentationsTransform(val_aug))\n",
    "    test_dataset = datasets.ImageFolder(Path(data_root) / 'test',\n",
    "                                       transform=AlbumentationsTransform(val_aug))\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model (ResNet-18, 1-channel in)\n",
    "\n",
    "def get_model():\n",
    "    model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "    model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training Loop, Early Stopping, Metrics\n",
    "\n",
    "def plot_metrics(train_hist, val_hist, name):\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.plot(train_hist, label='Train')\n",
    "    plt.plot(val_hist, label='Val')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(name)\n",
    "    plt.legend()\n",
    "    plt.title(f'{name} curve')\n",
    "    plt.savefig(f\"{PLOTS_DIR}/{name.lower()}_curve.png\")\n",
    "    plt.close()\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, epochs=10, patience=3):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=2, factor=0.5)\n",
    "\n",
    "    # Weighted loss for imbalance\n",
    "    class_counts = [0, 0]\n",
    "    for _, lbls in train_loader:\n",
    "        for l in lbls:\n",
    "            class_counts[l] += 1\n",
    "    weights = torch.FloatTensor([1/c if c > 0 else 1 for c in class_counts]).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n",
    "\n",
    "    best_acc, patience_cnt = 0, 0\n",
    "\n",
    "    # store metrics here\n",
    "    history = {\"train_losses\": [], \"val_losses\": [], \"train_accs\": [], \"val_accs\": []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # ---- TRAIN ----\n",
    "        model.train()\n",
    "        train_loss, correct, total = 0, 0, 0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast(enabled=scaler is not None):\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            if scaler:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            train_loss += loss.item() * imgs.size(0)\n",
    "            preds = outputs.argmax(1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        avg_train_loss = train_loss / total\n",
    "        train_acc = correct / total\n",
    "        history[\"train_losses\"].append(avg_train_loss)\n",
    "        history[\"train_accs\"].append(train_acc)\n",
    "\n",
    "        # ---- VALIDATION ----\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in val_loader:\n",
    "                imgs, labels = imgs.to(device), labels.to(device)\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * imgs.size(0)\n",
    "                preds = outputs.argmax(1)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / val_total\n",
    "        val_acc = val_correct / val_total\n",
    "        history[\"val_losses\"].append(avg_val_loss)\n",
    "        history[\"val_accs\"].append(val_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        scheduler.step(val_acc)\n",
    "\n",
    "        # early stopping\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            patience_cnt = 0\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        else:\n",
    "            patience_cnt += 1\n",
    "            if patience_cnt >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "    print(\"Best validation accuracy:\", best_acc)\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_results(train_losses, val_losses, train_accs, val_accs):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(12,5))\n",
    "\n",
    "    # Loss\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "    plt.plot(epochs, val_losses, label=\"Val Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Training vs Validation Loss\")\n",
    "\n",
    "    # Accuracy\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(epochs, train_accs, label=\"Train Accuracy\")\n",
    "    plt.plot(epochs, val_accs, label=\"Val Accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Training vs Validation Accuracy\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_results(history):\n",
    "    epochs = range(1, len(history[\"train_losses\"]) + 1)\n",
    "    plt.figure(figsize=(12,5))\n",
    "\n",
    "    # Loss\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(epochs, history[\"train_losses\"], label=\"Train Loss\")\n",
    "    plt.plot(epochs, history[\"val_losses\"], label=\"Val Loss\")\n",
    "    plt.xlabel(\"Epochs\"); plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training vs Validation Loss\")\n",
    "    plt.legend(); plt.grid(True)\n",
    "\n",
    "    # Accuracy\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(epochs, history[\"train_accs\"], label=\"Train Accuracy\")\n",
    "    plt.plot(epochs, history[\"val_accs\"], label=\"Val Accuracy\")\n",
    "    plt.xlabel(\"Epochs\"); plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Training vs Validation Accuracy\")\n",
    "    plt.legend(); plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model, history = train_model(\u001b[43mmodel\u001b[49m, train_loader, val_loader, device, epochs=\u001b[32m10\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# plot results\u001b[39;00m\n\u001b[32m      4\u001b[39m plot_training_results(history)\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "history = train_model(model, train_loader, val_loader, device, epochs=10)\n",
    "\n",
    "# plot results\n",
    "plot_training_results(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Evaluation: ROC, F1, Confusion Matrix, Report\n",
    "\n",
    "def plot_roc_curve(y_true, y_probs, filename):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_probs)\n",
    "    auc_val = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {auc_val:.2f})\")\n",
    "    plt.plot([0, 1], [0, 1], \"k--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Receiver operating characteristic\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "def eval_model(model, data_loader, device, split=\"Test\"):\n",
    "    model.eval()\n",
    "    test_correct, test_total = 0, 0\n",
    "    all_preds, all_labels, all_probs = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in data_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            preds = outputs.argmax(1)\n",
    "            test_correct += (preds == labels).sum().item()\n",
    "            test_total += labels.size(0)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs[:,1].cpu().numpy())\n",
    "    acc = test_correct / test_total\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    auc_val = roc_auc_score(all_labels, all_probs)\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    cr = classification_report(all_labels, all_preds, target_names=['NORMAL', 'PNEUMONIA'])\n",
    "    plot_roc_curve(all_labels, all_probs, f\"{PLOTS_DIR}/roc_{split.lower()}.png\")\n",
    "    print(f\"{split} accuracy: {acc:.4f} | F1: {f1:.4f} | AUC: {auc_val:.4f}\")\n",
    "    print(f\"{split} Confusion matrix:\\n{cm}\")\n",
    "    print(f\"{split} Classification report:\\n{cr}\")\n",
    "    # Save report\n",
    "    with open(f\"{REPORTS_DIR}/{split.lower()}_report.txt\", \"w\") as f:\n",
    "        f.write(f\"{split} accuracy: {acc:.4f} | F1: {f1:.4f} | AUC: {auc_val:.4f}\\n\")\n",
    "        f.write(f\"{split} Confusion matrix:\\n{cm}\\n\")\n",
    "        f.write(f\"{split} Classification report:\\n{cr}\\n\")\n",
    "    return acc, f1, auc_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cholponzhakshylykova/Desktop/SDS/SDS-CP021-pneumonia-detection/.venv/lib/python3.13/site-packages/albumentations/core/transforms_interface.py:310: UserWarning: The image is already gray.\n",
      "  target_function(ensure_contiguous_output(arg), **params),\n",
      "/var/folders/5s/xsxh_mvj2tz0s4q_6f7z01mm0000gn/T/ipykernel_7104/4237578504.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  imgs, labels = imgs.to(device), torch.tensor(labels).to(device)\n",
      "/var/folders/5s/xsxh_mvj2tz0s4q_6f7z01mm0000gn/T/ipykernel_7104/4237578504.py:37: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=scaler is not None):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=0.2126, Acc=0.9137 | Val Loss=0.4302, Acc=0.6875, F1=0.7619, AUC=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cholponzhakshylykova/Desktop/SDS/SDS-CP021-pneumonia-detection/.venv/lib/python3.13/site-packages/albumentations/core/transforms_interface.py:310: UserWarning: The image is already gray.\n",
      "  target_function(ensure_contiguous_output(arg), **params),\n",
      "/var/folders/5s/xsxh_mvj2tz0s4q_6f7z01mm0000gn/T/ipykernel_7104/4237578504.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  imgs, labels = imgs.to(device), torch.tensor(labels).to(device)\n",
      "/var/folders/5s/xsxh_mvj2tz0s4q_6f7z01mm0000gn/T/ipykernel_7104/4237578504.py:37: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=scaler is not None):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss=0.1244, Acc=0.9538 | Val Loss=0.0819, Acc=1.0000, F1=1.0000, AUC=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cholponzhakshylykova/Desktop/SDS/SDS-CP021-pneumonia-detection/.venv/lib/python3.13/site-packages/albumentations/core/transforms_interface.py:310: UserWarning: The image is already gray.\n",
      "  target_function(ensure_contiguous_output(arg), **params),\n",
      "/var/folders/5s/xsxh_mvj2tz0s4q_6f7z01mm0000gn/T/ipykernel_7104/4237578504.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  imgs, labels = imgs.to(device), torch.tensor(labels).to(device)\n",
      "/var/folders/5s/xsxh_mvj2tz0s4q_6f7z01mm0000gn/T/ipykernel_7104/4237578504.py:37: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=scaler is not None):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss=0.1112, Acc=0.9583 | Val Loss=0.1865, Acc=1.0000, F1=1.0000, AUC=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cholponzhakshylykova/Desktop/SDS/SDS-CP021-pneumonia-detection/.venv/lib/python3.13/site-packages/albumentations/core/transforms_interface.py:310: UserWarning: The image is already gray.\n",
      "  target_function(ensure_contiguous_output(arg), **params),\n",
      "/var/folders/5s/xsxh_mvj2tz0s4q_6f7z01mm0000gn/T/ipykernel_7104/4237578504.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  imgs, labels = imgs.to(device), torch.tensor(labels).to(device)\n",
      "/var/folders/5s/xsxh_mvj2tz0s4q_6f7z01mm0000gn/T/ipykernel_7104/4237578504.py:37: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=scaler is not None):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss=0.0940, Acc=0.9658 | Val Loss=0.7703, Acc=0.6250, F1=0.7273, AUC=0.9688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cholponzhakshylykova/Desktop/SDS/SDS-CP021-pneumonia-detection/.venv/lib/python3.13/site-packages/albumentations/core/transforms_interface.py:310: UserWarning: The image is already gray.\n",
      "  target_function(ensure_contiguous_output(arg), **params),\n",
      "/var/folders/5s/xsxh_mvj2tz0s4q_6f7z01mm0000gn/T/ipykernel_7104/4237578504.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  imgs, labels = imgs.to(device), torch.tensor(labels).to(device)\n",
      "/var/folders/5s/xsxh_mvj2tz0s4q_6f7z01mm0000gn/T/ipykernel_7104/4237578504.py:37: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=scaler is not None):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss=0.0776, Acc=0.9724 | Val Loss=0.1527, Acc=1.0000, F1=1.0000, AUC=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cholponzhakshylykova/Desktop/SDS/SDS-CP021-pneumonia-detection/.venv/lib/python3.13/site-packages/albumentations/core/transforms_interface.py:310: UserWarning: The image is already gray.\n",
      "  target_function(ensure_contiguous_output(arg), **params),\n",
      "/var/folders/5s/xsxh_mvj2tz0s4q_6f7z01mm0000gn/T/ipykernel_7104/4237578504.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  imgs, labels = imgs.to(device), torch.tensor(labels).to(device)\n",
      "/var/folders/5s/xsxh_mvj2tz0s4q_6f7z01mm0000gn/T/ipykernel_7104/4237578504.py:37: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=scaler is not None):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss=0.0622, Acc=0.9786 | Val Loss=0.1687, Acc=0.9375, F1=0.9412, AUC=1.0000\n",
      "Early stopping triggered.\n",
      "Best val accuracy: 1.0000\n",
      "\n",
      "--- Validation Set Performance ---\n",
      "Validation accuracy: 1.0000 | F1: 1.0000 | AUC: 1.0000\n",
      "Validation Confusion matrix:\n",
      "[[8 0]\n",
      " [0 8]]\n",
      "Validation Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      NORMAL       1.00      1.00      1.00         8\n",
      "   PNEUMONIA       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           1.00        16\n",
      "   macro avg       1.00      1.00      1.00        16\n",
      "weighted avg       1.00      1.00      1.00        16\n",
      "\n",
      "\n",
      "--- Test Set Performance ---\n",
      "Test accuracy: 0.8269 | F1: 0.8761 | AUC: 0.9409\n",
      "Test Confusion matrix:\n",
      "[[134 100]\n",
      " [  8 382]]\n",
      "Test Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      NORMAL       0.94      0.57      0.71       234\n",
      "   PNEUMONIA       0.79      0.98      0.88       390\n",
      "\n",
      "    accuracy                           0.83       624\n",
      "   macro avg       0.87      0.78      0.79       624\n",
      "weighted avg       0.85      0.83      0.81       624\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'grad_cam_available' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m eval_model(model, test_loader, device, split=\u001b[33m\"\u001b[39m\u001b[33mTest\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Grad-CAM visualizations\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mgrad_cam_available\u001b[49m:\n\u001b[32m     16\u001b[39m     gradcam_visualization(model, test_loader, device, out_dir=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPLOTS_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/gradcam\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAll metrics, plots, and reports are saved in \u001b[39m\u001b[33m'\u001b[39m\u001b[33mplots/\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mreports/\u001b[39m\u001b[33m'\u001b[39m\u001b[33m folders. Check TensorBoard logs in \u001b[39m\u001b[33m'\u001b[39m\u001b[33mruns/chest_xray\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'grad_cam_available' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Main: Train, Evaluate, Explain\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_loader, val_loader, test_loader = get_dataloaders(DATA_ROOT, batch_size=BATCH_SIZE)\n",
    "model = get_model().to(device)\n",
    "model = train_model(model, train_loader, val_loader, device, epochs=EPOCHS, patience=PATIENCE)\n",
    "# Load best model before test\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "print(\"\\n--- Validation Set Performance ---\")\n",
    "eval_model(model, val_loader, device, split=\"Validation\")\n",
    "print(\"\\n--- Test Set Performance ---\")\n",
    "eval_model(model, test_loader, device, split=\"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Final Model achieved 87% accuracy and an AUC of 0.92 on the test set. Data augmentation improved generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
